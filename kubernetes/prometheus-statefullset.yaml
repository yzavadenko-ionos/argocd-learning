apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus-server
  labels:
    app: prometheus-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
        environment: development
        tier: backend
    spec:
      # Define Prometheus and AlertManager containers
      containers:
        # Prometheus container
      - name: prometheus-server
        image: prom/prometheus:latest
        # Commands to run Prometheus with the specified configuration and data storage paths
        args:
        - '--config.file=/etc/prometheus/prometheus.yaml'
        - '--storage.tsdb.path=/prometheus'
        - '--storage.tsdb.retention.time=200h'
        - '--web.enable-lifecycle'
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus/prometheus.yaml
          subPath: prometheus.yaml
        - name: prometheus-rules
          mountPath: /etc/prometheus/rules
        - name: prometheus-data
          mountPath: /prometheus
        
        # AlertManager container
      - name: alertmanager
        image: prom/alertmanager:latest
        # Commands to run AlertManager with the specified configuration path
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yaml'
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager/alertmanager.yaml
          subPath: alertmanager.yaml

      # Define volumes for Prometheus and AlertManager configurations and data using Clusterwide ConfigMaps and Persistent Volume Claims
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-rules
        configMap:
          name: prometheus-rules
      - name: alertmanager-config
        configMap:
          name: alertmanager-config

  # Create a Persistent Volume Claim for Prometheus data storage
  # This ensures that Prometheus data is persisted across pod restarts and can be accessed by the Prometheus container.

  # If your StatefulSet pod (prometheus-server-0) dies and is rescheduled, Kubernetes must place it on the same node where the PV is available.
  # If thereâ€™s no node affinity, and the scheduler picks a different node, the pod cannot mount the volume, so it will stay pending until the original node becomes available.
  volumeClaimTemplates:
  - metadata:
      name: prometheus-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
